
#NLP #AI_session


## Transformers for Text Summarization

- Abstractive (generates new text)

	- BART
	- T5 
	- Pegasus 
	- GPT


- Extractive (no new text, picks and stitches text together) 

	- BERTSum

[BART Text Summarization vs. GPT-3 vs. BERT: An In-Depth Comparison](https://www.width.ai/post/bart-text-summarization)




# Cosine-similarity 

**![](https://lh7-us.googleusercontent.com/slidesz/AGV_vUes1X7DrnUw2t9odqNI2d-GW2sojWz54IcmAVWPhzzpHb3HKIEtJQeKmWZeG8zGxLRzFz8L46KZeBLlbAxJ0frY6dBSYpeYs11giaJ9IBU2ANd9I0hJFhGHQAIbX5Z2jOVy7_lHNnN5l1liLCQkilY8WA1-nD52=s2048?key=tdLJcBW7jZpL2kKM0R5dyA)![](https://lh7-us.googleusercontent.com/slidesz/AGV_vUdWlNzJsG_3BK8w3NqJAkjPnpkz0IcY0f7m2Chi3FAz8V3YG10b9Tsg3iarZWrzHtWGijsZb0-_gb3FqYI9YvsCYTtyWKZ0UR78oev0EvxYwRApHAB2bV5Hx4dS7HwDDhMVrSeyvrNcF5HuYKBLlfWn0uUhZB6c=s2048?key=tdLJcBW7jZpL2kKM0R5dyA)




Number of token should be same to input of the model , can be :

- Can be word
- set of words / sentences
- voice
should be unique , and is unit of conversation, arbitrary number and 




