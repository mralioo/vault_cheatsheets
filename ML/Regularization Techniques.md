#deeplearning 

Sources:
[How to Apply L1 and L2 Regularization Techniques to Keras Models](https://rukshanpramoditha.medium.com/how-to-apply-l1-and-l2-regularization-techniques-to-keras-models-da6249d8a469)
[How to Use Weight Decay to Reduce Overfitting of Neural Network in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/#:~:text=The%20most%20common%20type%20of,range%20between%200%20and%200.1.)

![Overview](../figures/Regularization%20Techniques-1.webp)

* L1 & L2 are weight/parameter regularization 
* it keeps **the weights of the neural network small (near zero)** by adding a **penalizing term to the loss function.**

*![](../figures/Regularization%20Techniques.webp)