
## What is XGBoost ? 
**XGBoost** is an acronym for **Extreme Gradient Boosting**. It is a powerful machine learning algorithm that can be used to solve classification and regression problems.
* It is an implementation of **gradient boosted trees** designed for speed and accuracy.
*  It has proved to be a highly effective machine learning algorithm extensively used in machine learning competitions
* It has high predictive power and is almost 10 times faster than other gradient boosting techniques. It also includes a variety of regularization parameters which reduces overfitting and improves overall performance. Hence, it is also known as **regularized boosting** technique.




* 
*![XGBoost](../figures/Pasted%20image%2020231115093015.png)


2/ When to Use XGBoost:
Use XGBoost when **you need a highly accurate predictive model**, especially in situations where other algorithms may struggle with complex patterns and relationships in the data.

3/ XGBoost is suitable for **both classification and regression tasks**, making it versatile for a wide range of applications. Consider using XGBoost for **structured data with features that exhibit nonlinear relationships and complex interactions.**


4/ Why Use XGBoost: High Predictive Accuracy: XGBoost **often outperforms other machine learning algorithms in terms of predictive accuracy**. It is known for its effectiveness in various data science competitions.


5/ Efficiency: XGBoost is efficient in **terms of training and prediction times**, thanks to optimizations such as **parallel and distributed computing**. 
Robustness: It is **robust to overfitting**, and you can control model complexity through regularization techniques.


6/ **Feature Importance**: XGBoost provides **feature importance scores**, helping you identify which features are most influential in making predictions. 
Handling Missing Data: XGBoost can **handle missing data natively**, simplifying data preprocessing.

7/ In the context of XGBoost, **ensemble learning** is achieved through the following key principles: Sequential Model Building: XGBoost builds an ensemble of decision trees sequentially. Each tree is created to correct the errors made by the previous trees.


8/ Weak Learners: **Each individual decision tree in XGBoost is considered a weak learner**. Weak learners are models that perform **slightly better than random guessing** but are not highly accurate on their own.





## References : 

Blog : [Naina Chaturvedi](https://twitter.com/NainaChaturved8/status/1722743268910235930)
